from langchain_core.messages import HumanMessage
from langgraph.graph.state import END, START, StateGraph

from chat_agh.nodes import (
    GenerationNode,
    InitialRetrievalNode,
    RetrievalNode,
    SupervisorNode,
)
from chat_agh.states import ChatState
from chat_agh.utils.agents_info import (
    RETRIEVAL_AGENTS,
    AgentDetails,
    AgentsInfo,
)
from chat_agh.utils.chat_history import ChatHistory


class ChatGraph:
    def __init__(self):
        """
        Initialize the chat graph pipeline.

        This method sets up the conversational flow using a StateGraph of ChatState.
        The graph defines how queries move between different nodes:
        - `initial_retrieval_node`: performs an initial retrieval step from predefined vector store indexes.
        - `supervisor_node`: decides whether additional retrieval is required or if generation can start directly.
        - `retrieval_node`: performs further retrieval if needed.
        - `generation_node`: generates the final response.

        The execution flow is as follows:
            START -> initial_retrieval_node -> supervisor_node
            supervisor_node -> retrieval_node OR generation_node
            retrieval_node -> generation_node -> END
        """
        self.graph = (
            StateGraph(ChatState)
            .add_node("initial_retrieval_node", InitialRetrievalNode())
            .add_node("supervisor_node", SupervisorNode())
            .add_node("retrieval_node", RetrievalNode())
            .add_node("generation_node", GenerationNode())
            .add_edge(START, "initial_retrieval_node")
            .add_edge("initial_retrieval_node", "supervisor_node")
            .add_conditional_edges(
                "supervisor_node",
                lambda state: (
                    "retrieval_node"
                    if state["retrieval_decision"]
                    else "generation_node"
                ),
            )
            .add_edge("retrieval_node", "generation_node")
            .add_edge("generation_node", END)
            .compile()
        )

    def query(self, question: str, **kwargs) -> str:
        """
        Query the chat graph with a plain text question.

        Args:
            question (str): The input user question.
            **kwargs: Optional keyword arguments passed downstream (not directly used here).

        Returns:
            str: The generated response after passing through the chat graph.
        """
        chat_history = ChatHistory(messages=[HumanMessage(question)])
        return self.invoke(chat_history)

    def invoke(self, chat_history: ChatHistory):
        """
        Execute the chat graph for a given chat history.

        Args:
            chat_history (ChatHistory): Conversation history including the latest user input.

        Returns:
            str: The final response generated by the graph.
        """
        state = ChatState(
            chat_history=chat_history, agents_info=self._get_agents_info()
        )
        return self.graph.invoke(state)["response"]

    def stream(self, chat_history: ChatHistory):
        """
        Stream responses from the chat graph as they are generated.

        Args:
            chat_history (ChatHistory): Conversation history including the latest user input.

        Yields:
            str: Incremental chunks of the generated response.
        """
        state = ChatState(
            chat_history=chat_history, agents_info=self._get_agents_info()
        )
        for response_chunk in self.graph.stream(state, stream_mode="custom"):
            yield response_chunk.content

    @staticmethod
    def _get_agents_info():
        """
        Retrieve metadata about available retrieval agents.

        Returns:
            AgentsInfo: Information containing details about each retrieval agent,
            including name, description, and cached history.
        """
        return AgentsInfo(
            agents_details=[
                AgentDetails(
                    name=agents_details.name,
                    description=agents_details.description,
                    cached_history=None,
                )
                for agents_details in RETRIEVAL_AGENTS
            ]
        )


if __name__ == "__main__":
    from chat_agh.utils.utils import logger

    chat_graph = ChatGraph()

    chat_history = ChatHistory(
        messages=[HumanMessage("Jak zostaÄ‡ studentem AGH?")]
    )
    logger.info("START")
    for c in chat_graph.stream(chat_history):
        print(c)
